{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONM5Q4SCe9Mr"
   },
   "source": [
    "# #Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ElRkQElWUMjG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "fhHRim2AUm4z",
    "outputId": "24458615-fbe8-4816-b069-be62cd290489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "#Load the IMDB dataset. You can load it using pandas as dataframe\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_Hn2f6VMP7"
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "5B5lHZPsVOXv",
    "outputId": "3de48d46-19b8-4701-e3bf-420a76594c8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shubham/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shubham/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/shubham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "for i, row in data.iterrows():\n",
    "\n",
    "    sentence = row['review']\n",
    "    \n",
    "    # remove html tags\n",
    "    sentence = BeautifulSoup(sentence, 'lxml').text\n",
    "\n",
    "    # remove urls\n",
    "    sentence = re.sub('https\\S+', '', sentence)\n",
    "    \n",
    "    # removing punctuations : how? -> how \n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "\n",
    "    tokens = word_tokenize(sentence, language='english')\n",
    "    # convert to lower case and remove non alphanumeric words\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum() and token not in stopword] \n",
    "\n",
    "    # perform stemming and lemmatization\n",
    "    tokens = [porter_stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
    "\n",
    "    data.at[i, 'review'] = \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "id": "DyaSkfcvYGXk",
    "outputId": "a2013d4b-5bee-4106-9681-6ef058c3c8e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of sentence is : 131.11266\n",
      "Class negative composes 25000 samples out of 50000 in dataset.\n",
      "Class positive composes 25000 samples out of 50000 in dataset.\n"
     ]
    }
   ],
   "source": [
    "# Statistics of data like avg length of sentence ,\n",
    "word_count = 0\n",
    "sent_count = 0\n",
    "for sentence in data['review']:\n",
    "    words = sentence.split()\n",
    "    word_count = word_count + len(words)\n",
    "    sent_count = sent_count + 1\n",
    "print(f'The average length of sentence is : {word_count/sent_count}')\n",
    "\n",
    "\n",
    "# Proposition of data w.r.t class labels\n",
    "for clas in data['sentiment'].value_counts().keys():\n",
    "    freq = data['sentiment'].value_counts()[clas]\n",
    "    print(f'Class {clas} composes {freq} samples out of {len(data)} in dataset.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FkJ-e2pUwun"
   },
   "source": [
    "# Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eVq-mN28U_J4"
   },
   "outputs": [],
   "source": [
    "# get reviews column from df\n",
    "reviews = data['review']\n",
    "\n",
    "# get labels column from df\n",
    "labels = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ljo5NquhXTXr"
   },
   "outputs": [],
   "source": [
    "# Use label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "\n",
    "# print(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wzG-C_EVWWET"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test (80% - 20%). \n",
    "# Use stratify in train_test_split so that both train and test have similar ratio of positive and negative samples.\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "for train_index, test_index in StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0).split(reviews, encoded_labels):\n",
    "      train_sentences, test_sentences, train_labels, test_labels = reviews[train_index], reviews[test_index], encoded_labels[train_index], encoded_labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1cllNfGmUr77"
   },
   "outputs": [],
   "source": [
    "# Use Count vectorizer to get frequency of the words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# max_features parameter : If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "vec_train_pos = CountVectorizer(max_features = 3000)\n",
    "X_train_pos = vec_train_pos.fit_transform(train_sentences[train_labels == 1].tolist())\n",
    "\n",
    "vec_train_neg = CountVectorizer(max_features = 3000)\n",
    "X_train_neg = vec_train_neg.fit_transform(train_sentences[train_labels == 0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qzRvPjWaWUnm"
   },
   "outputs": [],
   "source": [
    "# Use laplace smoothing for words in test set not present in vocab of train set\n",
    "train_word_list_pos = vec_train_pos.get_feature_names()\n",
    "train_count_list_pos = X_train_pos.toarray().sum(axis=0).tolist()\n",
    "\n",
    "train_word_list_neg = vec_train_neg.get_feature_names()\n",
    "train_count_list_neg = X_train_neg.toarray().sum(axis=0).tolist()\n",
    "\n",
    "pos_word_prob = dict(zip(train_word_list_pos, train_count_list_pos))\n",
    "neg_word_prob = dict(zip(train_word_list_neg, train_count_list_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE7pxWIYW1z0"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# Build the model. Don't use the model from sklearn\n",
    "def modelNB(sentence):\n",
    "    pos_prob = 0\n",
    "    neg_prob = 0\n",
    "    sum_pos_word = sum(list(pos_word_prob.values()))\n",
    "    sum_neg_word = sum(list(neg_word_prob.values()))\n",
    "\n",
    "  # print(type(sentence))\n",
    "    for word in sentence.split():\n",
    "    # print(word)\n",
    "        if word in pos_word_prob.keys():\n",
    "            pos_prob += math.log((pos_word_prob[word]+1)/(sum_pos_word+len(pos_word_prob)))\n",
    "        else : \n",
    "            pos_prob += math.log(1/(sum_pos_word+len(pos_word_prob)))\n",
    "    \n",
    "        if word in neg_word_prob.keys():\n",
    "            neg_prob += math.log((neg_word_prob[word]+1)/(sum_neg_word+len(neg_word_prob)))\n",
    "        else : \n",
    "            neg_prob += math.log(1/(sum_neg_word+len(neg_word_prob)))\n",
    "\n",
    "\n",
    "    if pos_prob > neg_prob:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "AtQSl1zvW4DD",
    "outputId": "20559e19-08b7-4fc8-db37-1db4df93cf90"
   },
   "outputs": [],
   "source": [
    "# Test the model on test set and report Accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "\n",
    "    if test_labels[i] == modelNB(sentence):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f'Accuracy : {(correct/total)*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlNql0acU7sa"
   },
   "source": [
    "# *LSTM* based Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SkqnvbUOXoN0"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 131 # based on avg sentence length\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UeycEg9nZAOF"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Mtw3w895ZP39",
    "outputId": "462d7f37-5de3-418d-d370-d2acfb96ad81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 131, 100)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "skmaDJMnZTzc",
    "outputId": "2aacf391-240d-4b45-aa75-bca3addf87f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 5316s 5s/step - loss: 0.4691 - accuracy: 0.7552 - val_loss: 0.3399 - val_accuracy: 0.8593\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 165s 147ms/step - loss: 0.2673 - accuracy: 0.8922 - val_loss: 0.3015 - val_accuracy: 0.8770\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 189s 168ms/step - loss: 0.2185 - accuracy: 0.9142 - val_loss: 0.3101 - val_accuracy: 0.8763\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 188s 167ms/step - loss: 0.1760 - accuracy: 0.9321 - val_loss: 0.3311 - val_accuracy: 0.8630\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 199s 177ms/step - loss: 0.1465 - accuracy: 0.9469 - val_loss: 0.3540 - val_accuracy: 0.8640\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "TjEhWEr5Zq7M",
    "outputId": "59b5d16f-c8dc-47b0-8c4f-c7bbc10f2803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.84      0.89      0.87      5000\n",
      "    negative       0.89      0.83      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on Test data\n",
    "# Get probabilities\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = (prediction>=0.5).astype(int)\n",
    "\n",
    "# Accuracy : one can use classification_report from sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['positive', 'negative']\n",
    "print(classification_report(test_labels, pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIICV-ySOYL0"
   },
   "source": [
    "## Get predictions for random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "m2RmfNL3OYL0",
    "outputId": "c7c61fc9-7fa6-402a-c340-cbf4077f7f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5233905 ]\n",
      " [0.4030612 ]\n",
      " [0.9637621 ]\n",
      " [0.32913542]]\n",
      "Predicted classes of sentences are : [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The movie was really really good, higly recommend\", \n",
    "            \"terrible terrible piece of art\",\n",
    "            \"caught my heart by surprise\",\n",
    "            \"acting was good but story made it look even taht baad\"]\n",
    "\n",
    "# convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "# pad the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get probabilities\n",
    "pred = model.predict(padded)\n",
    "print(pred)\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = (pred>=0.5).astype(int)\n",
    "\n",
    "# 0 means negative\n",
    "print(f'Predicted classes of sentences are : {pred_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do6H961jWeqN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Shrey_Shrivastava_17CS30034_Assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
